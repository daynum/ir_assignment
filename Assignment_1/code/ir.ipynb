{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing needed libraries and changing the directory to the corpus directory to process them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from collections import Counter\n",
    "from collections import OrderedDict\n",
    "import os\n",
    "import re\n",
    "path = os.getcwd()\n",
    "os.chdir(\"..\")\n",
    "#change to the corpus directory\n",
    "path = 'corpus'\n",
    "try:\n",
    "    os.chdir(path)\n",
    "except FileNotFoundError:\n",
    "    print(\"Directory: {0} does not exist\".format(path))\n",
    "except NotADirectoryError:\n",
    "    print(\"{0} is not a directory\".format(path))\n",
    "except PermissionError:\n",
    "    print(\"You do not have permissions to change to {0}\".format(path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating mappings from files names to docIDs and vice-versa.  \n",
    "Removing words such as: ACT 1, SCENE 1, redundant beginning metadata, single chars, punctuations.  \n",
    "Splitting the whole data into tokens, and storing them as a dictionary with key as title and value as a list of tokens.  \n",
    "Scanning the whole corpus, counting their frequencies, and removing top 30 most common words. So this is corpus specific stopword removal.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopwords removed:\n",
      "['the', 'and', 'to', 'of', 'you', 'my', 'that', 'in', 'is', 'not', 'he', 'with', 'me', 'it', 'for', 'be', 'his', 'your', 'as', 'this', 'but', 'have', 'thou', 'him', 'will', 'so', 'what', 'her', 'all', 'do']\n"
     ]
    }
   ],
   "source": [
    "#Precrocessing part, we are doing only stopword removal, stemming, and spelling correction if possible.\n",
    "#process each file and do basic tasks, such as\n",
    "#counting all words, mapping filenames\n",
    "numToTitle=[]\n",
    "titleToNum={}\n",
    "list_all_words=[]\n",
    "mappings={}\n",
    "i=0\n",
    "for filename in os.listdir():\n",
    "    file = open(filename)\n",
    "    title = file.readline()\n",
    "    title = title.strip()\n",
    "    numToTitle.append(title)\n",
    "    titleToNum[title]=i\n",
    "    i = i+1\n",
    "    \n",
    "    #feed all text data into a list, and count frequencies\n",
    "    data_raw = file.read()\n",
    "    \n",
    "    data_raw = re.sub('ACT\\s[1-9]', '', data_raw)\n",
    "    data_raw = re.sub('Scene\\s[1-9]', '', data_raw)\n",
    "    data_raw = re.sub('\\r?\\n+', ' ', data_raw)\n",
    "\n",
    "    temp_list = re.compile(\"=+\").split(data_raw)\n",
    "    full_data=\"\"\n",
    "    for partial_data in temp_list[3:len(temp_list)-1]:\n",
    "        partial_data = re.sub('[^\\w\\s]', ' ', partial_data)\n",
    "        partial_data = re.sub('\\s+', ' ', partial_data)\n",
    "        partial_data = re.sub('\\s.\\s', ' ', partial_data)\n",
    "        partial_data = partial_data.lower()\n",
    "        #at this point we have all the data extracted, cleaned\n",
    "        #lowercase, no punctuation, no single characters\n",
    "        full_data = full_data+partial_data\n",
    "\n",
    "        #tokenizing the full text of this document\n",
    "        tokenized_full_text = re.compile(\"\\s\").split(full_data)\n",
    "    list_all_words += tokenized_full_text\n",
    "    mappings[title] = tokenized_full_text\n",
    "\n",
    "##30 most common words are removed as stopwords\n",
    "freq = Counter(list_all_words)\n",
    "stopwords=[]\n",
    "for word, count in freq.most_common(30):\n",
    "    stopwords.append(word)\n",
    "#Now, we have the stopwords and our data with title in a dictionary\n",
    "print(\"Stopwords removed:\")\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the most time taking task with complexity: O( D * (N+N) ).  \n",
    "Hence, the complexity of the whole code would be: O( number of documents * average number of words in documents )  \n",
    "Creating a new list with stopwords removed, and then stemming all of them  \n",
    "Then assignning this cleaned and stemmed list of tokens back to the dictionary with title as key.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this block first chooses a document, then removes the stopword from it's text data\n",
    "# then it creates a set of words containing stemmed versions, and removing duplicacies as well\n",
    "# finally it puts back the cleaned and stemmed data back to the document's key in the dictionary\n",
    "\n",
    "discarded=0\n",
    "ps=PorterStemmer()\n",
    "for title in mappings:\n",
    "\n",
    "    # first removing stopwords\n",
    "    clean_list = [word for word in mappings[title] if word not in stopwords]\n",
    "\n",
    "    # now stemming the words\n",
    "    stemmed_list = set()\n",
    "    for word in clean_list:\n",
    "        if word != '':\n",
    "            stemmed_list.add(ps.stem(word))\n",
    "    # converting the set to list\n",
    "    final_clean_stemmed_list = list(stemmed_list)\n",
    "\n",
    "    # assigning back the cleaned and stemmed list to dictionary\n",
    "    mappings[title] = final_clean_stemmed_list\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating postings with keys as word tokens, and value as a list of docIDs.  \n",
    "Creating permuterm indices with adding a dollar sign and rotating the word.  \n",
    "Try catch block if the key doesn't exist, to add it.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this block creates postings and permuterm indices from earlier cleaned and stemmed data\n",
    "postings={}\n",
    "permuterms={}\n",
    "for i in range(0, len(numToTitle)):\n",
    "    for word in mappings[numToTitle[i]]:\n",
    "        dollar_word = word+'$'\n",
    "        for n in range(len(dollar_word)):\n",
    "            permuterms[dollar_word[n:] + dollar_word[:n]]=word\n",
    "        try:\n",
    "            if postings[word][len(postings[word])-1] != i:\n",
    "                postings[word].append(i)\n",
    "        except:\n",
    "            postings[word]=[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining lambda expressions and functions needed for further processing.  \n",
    "get_matched_keys (wildcard key with * at end): lambda expression to fetch keys which match with our wildcard searching format.  \n",
    "get_wild (key entered by user in wildcard format): function to handle different cases of wildcard searching formats.  \n",
    "sanitize_key (key entered by user): function to handle whether entered key has wildcard format or plain.  \n",
    "listAND (list 1, list 2): function to join 2 lists with AND operator.  \n",
    "listOR (list 1, list 2): function to join 2 lists with OR operator.  \n",
    "listNOT (list 1): function to negate a list with NOT operator.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining functions needed\n",
    "\n",
    "#lambda expression to get wildcard word matchings from permuterm indices\n",
    "get_matched_keys = lambda x: [pairs[1] for pairs in [(key, value) for key, value in permuterms.items() if key.startswith(x)]]\n",
    "\n",
    "# function to handle the different cases of wildcard formats\n",
    "def get_wild(key):\n",
    "    \n",
    "    if key[len(key)-1] == '*':\n",
    "        if key[0] == '*':\n",
    "            # *X* form look for X*\n",
    "            x=key[1:-1]\n",
    "        else:\n",
    "            # X* form look for $X*\n",
    "            x=key[:-1]\n",
    "            x='$'+x\n",
    "    elif key[0] == '*':\n",
    "        # *X form look up X$*\n",
    "        x=key[1:]\n",
    "        x=x+'$'\n",
    "    else:\n",
    "        # X*Y form look up Y$X*\n",
    "        x,y = key.split('*')\n",
    "        x=y+'$'+x\n",
    "\n",
    "    return get_matched_keys(x)\n",
    "\n",
    "# function to check whether key needs wildcard processing or not (sent to stemming)\n",
    "def sanitize_key(key):\n",
    "    if '*' in key:\n",
    "        return get_wild(key)\n",
    "    else:\n",
    "        return [ps.stem(key)]\n",
    "\n",
    "#Functions for boolean AND, OR, NOT operations on lists\n",
    "def listAND(list1, list2):\n",
    "    i=0\n",
    "    j=0\n",
    "    answer=[]\n",
    "    while i < (len(list1)) and j < (len(list2)):\n",
    "        if(list1[i]==list2[j]):\n",
    "            answer.append(list1[i])\n",
    "            i=i+1\n",
    "            j=j+1\n",
    "        elif list1[i] < list2[j]:\n",
    "            i=i+1\n",
    "        else:\n",
    "            j=j+1\n",
    "    return answer\n",
    "\n",
    "def listOR(list1, list2):\n",
    "    i=0\n",
    "    j=0\n",
    "    answer=set()\n",
    "    while i < (len(list1)):\n",
    "        answer.add(list1[i])\n",
    "        i=i+1\n",
    "    while j < (len(list2)):\n",
    "        answer.add(list2[j])\n",
    "        j=j+1\n",
    "    return list(answer)\n",
    "\n",
    "def listNOT(list1):\n",
    "    answer=[]\n",
    "    for i in range(len(numToTitle)):\n",
    "        if(i not in list1):\n",
    "            answer.append(i)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Block to just take input from the user.  \n",
    "Input must be in bracket free form.  \n",
    "Query first split on 'or' keyword, both sides are resolved and then added with listOR function.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter your query in bracket free form:\n"
     ]
    }
   ],
   "source": [
    "#process query, i.e. boolean expression parsing and wildcard searching in permuterm index\n",
    "#queries do not support brackets as of now, can parse but need time\n",
    "\n",
    "query = \"br* and ca*ar or calpurnia\"\n",
    "print(\"Enter your query in bracket free form:\")\n",
    "#query = input()\n",
    "query = query.lower()\n",
    "query='not in'\n",
    "query = query.split('or')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing the parts recieved after OR splitting. Only 'and', 'not' keywords remain.   \n",
    "Handling 'and' cases: either both keywords need to be negated, or only one of them needs to be negated, or none of them needs to be negated.  \n",
    "If the part doesn't contain 'and' then we directly procees to check for NOT and process the keyword.  \n",
    "Finally, join final list of lists with OR operator between all of them.\n",
    "\n",
    "Remove duplicates and sort them and display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['in']\n",
      "Entered keys not found in the data, please search with another key.\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "orlists=[]\n",
    "for part in query:\n",
    "    part=part.strip()\n",
    "    try:\n",
    "        if 'and' in part:\n",
    "            results=[]\n",
    "            part = part.split(' ')\n",
    "            if len(part)==3:\n",
    "                #no not expression\n",
    "                key_set_1 = sanitize_key(part[0])\n",
    "                key_set_2 = sanitize_key(part[2])\n",
    "                for key1 in key_set_1:\n",
    "                    for key2 in key_set_2:\n",
    "                        result = listAND(postings[key1], postings[key2])\n",
    "                        if result != '':\n",
    "                            orlists.append(result)\n",
    "            elif len(part)==5:\n",
    "                #both not expression\n",
    "                key_set_1 = sanitize_key(part[1])\n",
    "                key_set_2 = sanitize_key(part[4])\n",
    "                for key1 in key_set_1:\n",
    "                    for key2 in key_set_2:\n",
    "                        result = listAND(listNOT(postings[key1]), listNOT(postings[key2]))\n",
    "                        if result != '':\n",
    "                            orlists.append(result)\n",
    "            else:\n",
    "                # one key has not operator\n",
    "                if part[0]=='not':\n",
    "                    key_set_1 = sanitize_key(part[1])\n",
    "                    key_set_2 = sanitize_key(part[3])\n",
    "                    for key1 in key_set_1:\n",
    "                        for key2 in key_set_2:\n",
    "                            result = listAND(listNOT(postings[key1]), postings[key2])\n",
    "                            if result != '':\n",
    "                                orlists.append(result)\n",
    "\n",
    "                else:\n",
    "                    key_set_1 = sanitize_key(part[0])\n",
    "                    key_set_2 = sanitize_key(part[3])\n",
    "                    for key1 in key_set_1:\n",
    "                        for key2 in key_set_2:\n",
    "                            result = listAND(postings[key1], listNOT(postings[key2]))\n",
    "                            if result != '':\n",
    "                                orlists.append(result)\n",
    "        else:\n",
    "            result=[]\n",
    "            part = part.split(' ')\n",
    "            if 'not' in part:\n",
    "                key_set_1 = sanitize_key(part[1])\n",
    "                print(key_set_1)\n",
    "                for key in key_set_1:\n",
    "                    result.extend(listNOT(postings[key]))\n",
    "                    print(listNOT(postings[key]))\n",
    "            else:\n",
    "                key_set_1 = sanitize_key(part[1])\n",
    "                #print(key_set_1)\n",
    "                for key in key_set_1:\n",
    "                    result.extend(postings[key])\n",
    "                    #print(result)\n",
    "            if result != '':\n",
    "                orlists.append(result)\n",
    "    except:\n",
    "        print(\"Entered keys not found in the data, please search with another key.\")\n",
    "\n",
    "#now we have lists separated by OR operator\n",
    "ans=[]\n",
    "for list1 in orlists:\n",
    "    for list2 in orlists:\n",
    "        ans.extend(listOR(list1, list2))\n",
    "\n",
    "ans = list(OrderedDict.fromkeys(ans))\n",
    "ans.sort()\n",
    "print(ans)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fcf1d46d271c46101d6967829d4a5f475342a2ce08e4944f989fbcdc9bb23690"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
